# -*- coding: utf-8 -*-
"""
DOCX RAG Chatbot â€” LangChain + FAISS + Streamlit
- å¤šæ–‡ä»¶ä¸Šä¼ ï¼ˆ.docxï¼‰ï¼Œæœ‰æ•ˆæ€§æ ¡éªŒï¼ˆé˜² BadZipFileï¼‰
- ä¸­æ–‡å‹å¥½åˆ‡åˆ†
- ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ + MMR å»å†—ä½™
- æ„å»ºå‘é‡ç´¢å¼•å¸¦è¿›åº¦æ¡ã€ç­‰å¾…æç¤º
- ä¼šè¯ç¼“å­˜ï¼ˆä¸€æ¬¡ç´¢å¼•ï¼Œå¤šè½®é—®ç­”ï¼‰
- å®‰å…¨è¯»å–é…ç½®ï¼šä¼˜å…ˆç¯å¢ƒå˜é‡ï¼Œå†å°è¯• st.secrets
"""

import os
from typing import List, Optional

import streamlit as st
from langchain_community.document_loaders import Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema import AIMessage

# -----------------------------
# é¡µé¢åŸºç¡€é…ç½®ä¸æ ·å¼
# -----------------------------
st.set_page_config(
    page_title="DOCX RAG Chatbot",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# è½»åº¦ç¾åŒ–
st.markdown("""
<style>
/* é¡¶éƒ¨æ ‡é¢˜ç¼©å°ç•™ç™½ */
.block-container {padding-top: 1.3rem; padding-bottom: 2rem;}
/* èŠå¤©æ¶ˆæ¯å†…å®¹æ›´æ˜“è¯» */
.stChatMessage p {font-size:1.02rem; line-height:1.6;}
/* ä¾§æ æ ‡é¢˜é—´è· */
section[data-testid="stSidebar"] .st-emotion-cache-1vt4y43 {margin-bottom: 0.5rem;}
/* ä»£ç å—æ›´ç´§å‡‘ */
code, pre {font-size: 0.92rem;}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§  DOCX RAG Chatbot")
st.caption("LangChain + FAISS + Streamlitï¼ˆæ”¯æŒå¤šæ–‡æ¡£ã€MMRã€ç›¸ä¼¼åº¦é˜ˆå€¼ã€è¿›åº¦æ¡ï¼‰")

# -----------------------------
# å·¥å…·å‡½æ•°
# -----------------------------
def read_secret(name: str, default: Optional[str] = None) -> Optional[str]:
    """å®‰å…¨è¯»å–é…ç½®ï¼šç¯å¢ƒå˜é‡ä¼˜å…ˆï¼Œå†å°è¯• st.secretsï¼›éƒ½æ— åˆ™ç»™é»˜è®¤å€¼"""
    v = os.environ.get(name)
    if v:
        return v
    try:
        return st.secrets[name]
    except Exception:
        return default

def is_valid_docx_bytes(b: bytes) -> bool:
    """docx æœ¬è´¨æ˜¯ zipï¼Œå‰ä¸¤ä¸ªå­—èŠ‚ä¸º 'PK'"""
    return b[:2] == b"PK"

def load_docs_from_uploads(uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile]):
    docs = []
    tmp_dir = "uploaded_docx"
    os.makedirs(tmp_dir, exist_ok=True)
    for uf in uploaded_files:
        raw = uf.read()
        if not is_valid_docx_bytes(raw):
            st.warning(f"âš ï¸ `{uf.name}` ä¸æ˜¯æœ‰æ•ˆçš„ .docxï¼ˆæˆ–å·²æŸåï¼‰ï¼Œå·²è·³è¿‡ã€‚")
            continue
        path = os.path.join(tmp_dir, uf.name)
        with open(path, "wb") as f:
            f.write(raw)
        docs.extend(Docx2txtLoader(path).load())
    return docs

def split_documents(docs, chunk_size: int, chunk_overlap: int):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", "ã€", " ", ""],
    )
    return splitter.split_documents(docs)

def build_vectordb_with_progress(splits, embed_model: str, base_url: str, api_key: str) -> FAISS:
    if not splits:
        raise ValueError("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡æœ¬ç‰‡æ®µã€‚")

    progress = st.progress(0.0, text="æ­£åœ¨å‘é‡åŒ–ä¸å»ºç«‹ç´¢å¼•â€¦ï¼ˆå¯åŠ¨ä¸­ï¼‰")
    status = st.empty()

    embeddings = OpenAIEmbeddings(model=embed_model, base_url=base_url, api_key=api_key)

    total = len(splits)
    batch_size = max(16, min(128, total // 10 or 16))

    first = splits[:batch_size]
    status.info(f"ç´¢å¼•åˆå§‹åŒ–â€¦ï¼ˆ1/{(total-1)//batch_size + 1} æ‰¹ï¼‰")
    vectordb = FAISS.from_documents(first, embeddings)
    built = len(first)
    progress.progress(min(0.08 + 0.80 * (built/total), 0.95), text=f"å·²å»ºç«‹ {built}/{total} ä¸ªç‰‡æ®µâ€¦")

    while built < total:
        end = min(built + batch_size, total)
        batch = splits[built:end]
        status.info(f"å¢é‡ç´¢å¼•â€¦ï¼ˆ{end//batch_size + (1 if end%batch_size else 0)}/{(total-1)//batch_size + 1} æ‰¹ï¼‰")
        # âœ… ä¿®å¤ï¼šæ–°ç‰ˆ LangChain ä¸å†éœ€è¦ä¼  embeddings
        vectordb.add_documents(batch)
        built = end
        progress.progress(min(0.08 + 0.80 * (built/total), 0.98), text=f"å·²å»ºç«‹ {built}/{total} ä¸ªç‰‡æ®µâ€¦")

    progress.progress(1.0, text="ç´¢å¼•å®Œæˆ âœ…")
    progress.empty()
    status.empty()
    return vectordb

def retrieve_docs(vdb: FAISS, query: str, top_k: int, use_mmr: bool, dist_threshold: Optional[float]):
    # å…ˆå°è¯•â€œè·ç¦»é˜ˆå€¼â€è¿‡æ»¤ï¼ˆFAISS åˆ†æ•°æ˜¯è·ç¦»ï¼šè¶Šå°è¶Šç›¸ä¼¼ï¼‰
    if dist_threshold is not None:
        cands = vdb.similarity_search_with_score(query, k=max(20, top_k * 5))
        picked = []
        for d, dist in cands:
            if dist <= dist_threshold:
                picked.append(d)
            if len(picked) >= top_k:
                break
        if picked:
            return picked

    # å¦åˆ™é€€å› MMR æˆ–æ™®é€šæ£€ç´¢
    if use_mmr:
        retr = vdb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": top_k, "fetch_k": max(10, top_k * 4), "lambda_mult": 0.5},
        )
        return retr.get_relevant_documents(query)
    else:
        retr = vdb.as_retriever(search_kwargs={"k": top_k})
        return retr.get_relevant_documents(query)

def format_context(docs) -> str:
    return "\n\n".join((d.page_content or "").strip() for d in docs)

def make_llm(model: str, base_url: str, api_key: str) -> ChatOpenAI:
    return ChatOpenAI(model=model, temperature=0.2, base_url=base_url, api_key=api_key)

# -----------------------------
# ä¾§è¾¹æ ï¼ˆç¡®ä¿æ˜¾ç¤ºï¼‰
# -----------------------------
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")

    default_base_url = read_secret("OPENAI_BASE_URL", "https://www.dmxapi.cn/v1")
    default_api_key  = read_secret("OPENAI_API_KEY", "")
    default_embed    = read_secret("EMBEDDING_MODEL", "text-embedding-3-small")
    default_llm      = read_secret("LLM_MODEL", "gpt-4o-mini")

    base_url   = st.text_input("Base URLï¼ˆOpenAI å…¼å®¹ï¼‰", value=default_base_url, help="ä¾‹å¦‚ï¼šhttps://www.dmxapi.cn/v1")
    api_key    = st.text_input("API Keyï¼ˆä¸ä¼šä¿å­˜ï¼‰", type="password", value=default_api_key)
    llm_model  = st.selectbox("å¯¹è¯æ¨¡å‹", ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"],
                               index=0 if default_llm not in ["gpt-4o", "gpt-4.1-mini"]
                               else ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"].index(default_llm))
    embed_model = st.text_input("Embedding æ¨¡å‹", value=default_embed)

    st.divider()
    st.subheader("æ£€ç´¢ä¸åˆ‡åˆ†")
    top_k = st.slider("Top-Kï¼ˆè¿”å›ç‰‡æ®µæ•°ï¼‰", 1, 10, 4, 1)
    use_mmr = st.checkbox("å¯ç”¨ MMR å»å†—ä½™æ£€ç´¢", value=True)
    use_threshold = st.checkbox("å¯ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼ˆåŸºäº FAISS è·ç¦»ï¼‰", value=True)
    dist_threshold = st.slider("è·ç¦»é˜ˆå€¼ï¼ˆè¶Šå°è¶Šä¸¥æ ¼ï¼‰", 0.10, 1.00, 0.45, 0.05, disabled=not use_threshold)
    chunk_size = st.number_input("åˆ‡ç‰‡å¤§å° chunk_size", 200, 2000, 800, 50)
    chunk_overlap = st.number_input("åˆ‡ç‰‡é‡å  chunk_overlap", 0, 800, 100, 10)

    st.divider()
    clear_btn = st.button("ğŸ§¹ æ¸…ç©ºä¼šè¯ä¸ç´¢å¼•", use_container_width=True)

# -----------------------------
# ä¼šè¯çŠ¶æ€
# -----------------------------
if clear_btn:
    for k in list(st.session_state.keys()):
        del st.session_state[k]
    st.rerun()

if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
if "ready" not in st.session_state:
    st.session_state.ready = False
if "messages" not in st.session_state:
    st.session_state.messages = []
if "sources" not in st.session_state:
    st.session_state.sources = []
if "files" not in st.session_state:
    st.session_state.files = []

# -----------------------------
# ç³»ç»Ÿ Prompt
# -----------------------------
SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·ä¼˜å…ˆåŸºäºä¸‹æ–¹â€œèµ„æ–™ç‰‡æ®µâ€å›ç­”ï¼›"
    "è‹¥èµ„æ–™ä¸è¶³ï¼Œè¯·å¦‚å®è¯´æ˜å¹¶ç»™å‡ºéœ€è¦è¡¥å……çš„å†…å®¹ã€‚å›ç­”æ¡ç†æ¸…æ™°ã€ç®€æ´ã€‚"
)
PROMPT = ChatPromptTemplate.from_template(
    """{sys}

èµ„æ–™ç‰‡æ®µï¼š
{context}

é—®é¢˜ï¼š
{question}

è¯·ç”¨ä¸­æ–‡ç®€æ´ä½œç­”ï¼›è‹¥èµ„æ–™ä¸è¶³è¯·ç›´è¯´ã€‚"""
)

# -----------------------------
# ä¸Šä¼ ä¸æ„å»ºç´¢å¼•
# -----------------------------
st.subheader("ğŸ“¤ ä¸Šä¼  DOCX æ–‡æ¡£ï¼ˆå¯å¤šé€‰ï¼‰")
uploaded_files = st.file_uploader("ä»…æ”¯æŒ .docx", type=["docx"], accept_multiple_files=True)

c1, c2 = st.columns([1.2, 2.8])
with c1:
    build_btn = st.button("ğŸš€ æ„å»º / æ›´æ–°ç´¢å¼•", type="primary", use_container_width=True)
with c2:
    if st.session_state.ready and st.session_state.vectordb is not None:
        st.success(f"ç´¢å¼•å·²å°±ç»ªã€‚æœ¬æ¬¡ä¼šè¯å…±è½½å…¥ {len(st.session_state.files)} ä¸ªæ–‡ä»¶ã€‚")

if build_btn:
    if not api_key:
        st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥ API Keyã€‚")
    elif not uploaded_files:
        st.error("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ª .docx æ–‡ä»¶ã€‚")
    else:
        with st.spinner("æ­£åœ¨åŠ è½½ä¸åˆ‡åˆ†æ–‡æ¡£â€¦"):
            docs = load_docs_from_uploads(uploaded_files)
            st.session_state.files = [f.name for f in uploaded_files if f is not None]
            if not docs:
                st.error("æ²¡æœ‰æˆåŠŸåŠ è½½çš„ .docx æ–‡æ¡£ã€‚")
            else:
                splits = split_documents(docs, int(chunk_size), int(chunk_overlap))
                st.info(f"å·²åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µã€‚")
        try:
            st.session_state.vectordb = build_vectordb_with_progress(
                splits,
                embed_model=embed_model,
                base_url=base_url,
                api_key=api_key,
            )
            st.session_state.ready = True
            st.success("âœ… å‘é‡ç´¢å¼•å°±ç»ªï¼å¯ä»¥å¼€å§‹æé—®ã€‚")
        except Exception as e:
            st.exception(e)

# -----------------------------
# èŠå¤©åŒº
# -----------------------------
st.subheader("ğŸ’¬ æé—®åŒº")

# å±•ç¤ºå†å²æ¶ˆæ¯
for role, content in st.session_state.messages:
    with st.chat_message(role):
        st.markdown(content)

user_q = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜â€¦")
if user_q:
    st.session_state.messages.append(("user", user_q))
    with st.chat_message("user"):
        st.markdown(user_q)

    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­â€¦"):
            llm = make_llm(llm_model, base_url, api_key)
            use_rag = st.session_state.ready and (st.session_state.vectordb is not None)
            if use_rag:
                hits = retrieve_docs(
                    st.session_state.vectordb,
                    user_q,
                    top_k=int(top_k),
                    use_mmr=bool(use_mmr),
                    dist_threshold=(float(dist_threshold) if use_threshold else None),
                )
                context = format_context(hits) if hits else "(æœªæ£€ç´¢åˆ°ç›¸å…³èµ„æ–™)"
            else:
                hits = []
                context = "(æœªæ„å»ºç´¢å¼•ï¼Œæœ¬æ¬¡ä¸ºæ— æ£€ç´¢å›ç­”)"

            msgs = PROMPT.format_messages(sys=SYSTEM_PROMPT, context=context, question=user_q)
            try:
                resp = llm.invoke(msgs)
                answer = resp.content if isinstance(resp, AIMessage) else str(resp)
            except Exception as e:
                answer = f"æ¨¡å‹è°ƒç”¨å‡ºé”™ï¼š{e}"
            st.markdown(answer)
            st.session_state.messages.append(("assistant", answer))

        # å±•ç¤ºæ¥æº
        if hits:
            with st.expander("ğŸ“š æ¥æºæ–‡æ¡£ / å‘½ä¸­ç‰‡æ®µ", expanded=False):
                for i, d in enumerate(hits, 1):
                    src = d.metadata.get("source", "unknown")
                    snippet = (d.page_content or "").strip()
                    st.markdown(f"**[{i}] {src}**")
                    st.code(snippet[:1200])
        else:
            if use_rag:
                st.info("æœªæ£€ç´¢åˆ°ç›¸å…³ç‰‡æ®µï¼Œè¯·å°è¯•é™ä½é˜ˆå€¼ã€å¢å¤§ Top-Kï¼Œæˆ–ä¸Šä¼ æ›´ç›¸å…³çš„æ–‡æ¡£ã€‚")
            else:
                st.info("æç¤ºï¼šæ„å»ºç´¢å¼•åå¯è·å¾—æ›´å¯é çš„åŸºäºèµ„æ–™çš„å›ç­”ã€‚")

